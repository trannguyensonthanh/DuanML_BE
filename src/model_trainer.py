# src/model_trainer.py
import os
import cv2
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time

# Import c√°c th∆∞ vi·ªán Machine Learning c≈© c·ªßa b·∫°n
from sklearn.model_selection import train_test_split, RandomizedSearchCV  # <-- D√πng c√°i n√†y cho nhanh
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from xgboost import XGBClassifier

from .feature_extractor import FeatureExtractor
from .utils import setup_logger, ensure_dir

logger = setup_logger("Trainer")


class TrashClassifier:
    def __init__(self, data_dir, model_path="models/stacking_model.pkl"):
        self.data_dir = data_dir
        self.model_path = model_path
        self.results_dir = "models/grid_search_results"
        self.features_path = "features/features.joblib"
        self.labels_path = "features/labels.joblib"

        ensure_dir(self.results_dir)
        ensure_dir("features")
        ensure_dir("models")

        self.extractor = FeatureExtractor()
        self.label_encoder = LabelEncoder()

    def load_and_extract_features(self):
        # 1. Ki·ªÉm tra cache
        if os.path.exists(self.features_path) and os.path.exists(self.labels_path):
            logger.info(f"‚úÖ T√¨m th·∫•y file features cache. ƒêang t·∫£i...")
            return joblib.load(self.features_path), joblib.load(self.labels_path)

        # 2. N·∫øu ch∆∞a c√≥, tr√≠ch xu·∫•t
        logger.info("‚è≥ B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng...")
        X = []
        y_text = []

        if not os.path.exists(self.data_dir):
            logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {self.data_dir}")
            return [], []

        classes = sorted(os.listdir(self.data_dir))

        # L·∫•y danh s√°ch ·∫£nh
        all_files = []
        for label in classes:
            class_path = os.path.join(self.data_dir, label)
            if not os.path.isdir(class_path): continue
            for f in os.listdir(class_path):
                all_files.append((os.path.join(class_path, f), label))

        # Tr√≠ch xu·∫•t (D√πng Extractor m·ªõi c·ªßa b·∫°n: HOG ho·∫∑c ResNet ƒë·ªÅu ƒë∆∞·ª£c)
        for img_path, label in tqdm(all_files, desc="Processing Images"):
            img = cv2.imread(img_path)
            if img is None: continue

            # Feature Extractor t·ª± lo ph·∫ßn CLAHE/Resize b√™n trong
            vector = self.extractor.extract(img)

            if vector is not None:
                X.append(vector)
                y_text.append(label)

        X = np.array(X, dtype=np.float32)
        y = np.array(y_text)

        # L∆∞u cache
        joblib.dump(X, self.features_path)
        joblib.dump(y, self.labels_path)
        logger.info(f"üíæ ƒê√£ l∆∞u cache features.")
        return X, y

    def train(self):
        start_total_time = time.time()

        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
        X, y_text = self.load_and_extract_features()
        if len(X) == 0: return

        y = self.label_encoder.fit_transform(y_text)
        joblib.dump(self.label_encoder, "models/label_encoder.pkl")

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        logger.info(f"üìä Train shape: {X_train.shape} | Test shape: {X_test.shape}")

        # C·∫•u h√¨nh chung cho RandomizedSearch (nhanh h∆°n GridSearch)
        # n_iter=10 nghƒ©a l√† ch·ªâ th·ª≠ ng·∫´u nhi√™n 10 t·ªï h·ª£p tham s·ªë -> Ti·∫øt ki·ªám 90% th·ªùi gian
        N_ITER_SEARCH = 10
        CV_FOLDS = 3

        # ==========================================
        # 1. SVM (ƒê√£ th√™m PCA ƒë·ªÉ ch·∫°y nhanh h∆°n)
        # ==========================================
        svm_path = "models/best_svm_model.pkl"
        if os.path.exists(svm_path):
            logger.info("‚úÖ Load SVM t·ª´ file...")
            best_svm = joblib.load(svm_path)
        else:
            logger.info("üöÄ ƒêang tune SVM (Fast Mode)...")
            # Pipeline: Scaler -> PCA (gi·∫£m chi·ªÅu) -> SVM
            svm_pipe = Pipeline([
                ('scaler', StandardScaler()),
                ('pca', PCA(n_components=0.95)),  # Gi·ªØ 95% th√¥ng tin, b·ªè nhi·ªÖu -> SVM ch·∫°y nhanh g·∫•p b·ªôi
                ('clf', SVC(probability=True, class_weight='balanced', cache_size=1000))
            ])
            svm_params = {
                'clf__C': [1, 10, 100],
                'clf__gamma': ['scale', 0.01, 0.001],
                'clf__kernel': ['rbf']  # RBF l√† t·ªët nh·∫•t nh∆∞ng n·∫∑ng, nh·ªù c√≥ PCA n√™n s·∫Ω ·ªïn
            }
            # n_jobs=-1 ƒë·ªÉ ch·∫°y ƒëa lu·ªìng
            svm_search = RandomizedSearchCV(svm_pipe, svm_params, n_iter=N_ITER_SEARCH, cv=CV_FOLDS, n_jobs=-1,
                                            verbose=1, scoring='f1_macro')
            svm_search.fit(X_train, y_train)
            best_svm = svm_search.best_estimator_
            joblib.dump(best_svm, svm_path)
            logger.info(f"üéØ SVM xong. F1: {svm_search.best_score_:.4f}")

        # ==========================================
        # 2. Random Forest
        # ==========================================
        rf_path = "models/best_rf_model.pkl"
        if os.path.exists(rf_path):
            logger.info("‚úÖ Load Random Forest t·ª´ file...")
            best_rf = joblib.load(rf_path)
        else:
            logger.info("üöÄ ƒêang tune Random Forest...")
            rf_pipe = Pipeline([('clf', RandomForestClassifier(random_state=42, n_jobs=-1))])
            rf_params = {
                'clf__n_estimators': [100, 200, 300],
                'clf__max_depth': [10, 20, None],
                'clf__min_samples_split': [2, 5]
            }
            rf_search = RandomizedSearchCV(rf_pipe, rf_params, n_iter=N_ITER_SEARCH, cv=CV_FOLDS, n_jobs=-1, verbose=1,
                                           scoring='f1_macro')
            rf_search.fit(X_train, y_train)
            best_rf = rf_search.best_estimator_
            joblib.dump(best_rf, rf_path)
            logger.info(f"üéØ RF xong. F1: {rf_search.best_score_:.4f}")

        # ==========================================
        # 3. XGBoost
        # ==========================================
        xgb_path = "models/best_xgb_model.pkl"
        if os.path.exists(xgb_path):
            logger.info("‚úÖ Load XGBoost t·ª´ file...")
            best_xgb = joblib.load(xgb_path)
        else:
            logger.info("üöÄ ƒêang tune XGBoost...")
            # tree_method='hist' gi√∫p train c·ª±c nhanh
            xgb_pipe = Pipeline([('clf', XGBClassifier(eval_metric='mlogloss', tree_method='hist', n_jobs=-1))])
            xgb_params = {
                'clf__n_estimators': [100, 200, 300],
                'clf__learning_rate': [0.01, 0.1, 0.2],
                'clf__max_depth': [3, 6, 10]
            }
            xgb_search = RandomizedSearchCV(xgb_pipe, xgb_params, n_iter=N_ITER_SEARCH, cv=CV_FOLDS, n_jobs=-1,
                                            verbose=1, scoring='f1_macro')
            xgb_search.fit(X_train, y_train)
            best_xgb = xgb_search.best_estimator_
            joblib.dump(best_xgb, xgb_path)
            logger.info(f"üéØ XGBoost xong. F1: {xgb_search.best_score_:.4f}")

        # ==========================================
        # 4. STACKING (G·ªôp 3 √¥ng th·∫ßn l·∫°i)
        # ==========================================
        logger.info("=" * 20 + " HU·∫§N LUY·ªÜN STACKING FINAL " + "=" * 20)
        estimators = [
            ('svm', best_svm),
            ('rf', best_rf),
            ('xgb', best_xgb)
        ]

        # Meta-learner l√† Logistic Regression ƒë·ªÉ t·ªïng h·ª£p √Ω ki·∫øn
        stacking_model = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(max_iter=1000),
            cv=3,
            n_jobs=-1,
            passthrough=False
        )

        logger.info("üöÄ ƒêang fit Stacking Model...")
        stacking_model.fit(X_train, y_train)

        # 5. ƒê√°nh gi√°
        logger.info("üìä ƒê√°nh gi√° tr√™n t·∫≠p Test...")
        y_pred = stacking_model.predict(X_test)

        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')

        print("\n" + "=" * 50)
        print(f"üèÜ ƒê·ªò CH√çNH X√ÅC STACKING: {acc * 100:.2f}%")
        print(f"üéØ MACRO F1-SCORE:       {f1 * 100:.2f}%")
        print("=" * 50)
        print("\nB√ÅO C√ÅO CHI TI·∫æT:")
        print(classification_report(y_test, y_pred, target_names=self.label_encoder.classes_))

        self.plot_confusion_matrix(y_test, y_pred, self.label_encoder.classes_)

        # L∆∞u model cu·ªëi c√πng
        joblib.dump(stacking_model, self.model_path)
        total_time = (time.time() - start_total_time) / 60
        logger.info(f"üéâ Ho√†n t·∫•t to√†n b·ªô qu√° tr√¨nh trong {total_time:.2f} ph√∫t.")

    def plot_confusion_matrix(self, y_true, y_pred, classes):
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=classes, yticklabels=classes)
        plt.title('Confusion Matrix')
        plt.ylabel('Th·ª±c t·∫ø')
        plt.xlabel('D·ª± ƒëo√°n')
        plt.tight_layout()
        plt.savefig('models/confusion_matrix.png')
        plt.close()