import streamlit as st
import cv2
import numpy as np
import torch
import joblib
import sys
from segment_anything import sam_model_registry, SamPredictor
from PIL import Image
from streamlit_drawable_canvas import st_canvas
import io

# =================================================================
# == C·∫§U H√åNH & T·∫¢I MODEL
# =================================================================

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import c√°c module t·ª´ src
sys.path.append("./src")
from src.feature_extractor import FeatureExtractor

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n v√† model
SAM_MODEL_TYPE = "vit_b"
SAM_CHECKPOINT = "./models/sam/sam_vit_b_01ec64.pth"
CLASSIFIER_MODEL = "./models/stacking_model.pkl"
LABEL_ENCODER = "./models/label_encoder.pkl" 

st.set_page_config(layout="wide", page_title="H·ªá th·ªëng Ph√¢n lo·∫°i R√°c th·∫£i Pro")

@st.cache_resource
def load_models():
    """T·∫£i v√† cache t·∫•t c·∫£ c√°c model c·∫ßn thi·∫øt (SAM v√† Classifier)."""
    # T·∫£i SAM
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
        sam.to(device=device)
        predictor = SamPredictor(sam)
    except FileNotFoundError:
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file model SAM t·∫°i '{SAM_CHECKPOINT}'.")
        return None, None, None, None

    # T·∫£i Classifier (M√¥ h√¨nh h·ªçc m√°y)
    try:
        classifier = joblib.load(CLASSIFIER_MODEL)
        label_encoder = joblib.load(LABEL_ENCODER)
    except FileNotFoundError:
        st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y model ph√¢n lo·∫°i t·∫°i '{CLASSIFIER_MODEL}' ho·∫∑c '{LABEL_ENCODER}'. Vui l√≤ng train model tr∆∞·ªõc.")
        return None, None, None, None
        
    # Kh·ªüi t·∫°o b·ªô tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
    feature_extractor = FeatureExtractor()

    return predictor, classifier, label_encoder, feature_extractor

# =================================================================
# == H√ÄM X·ª¨ L√ù (HELPER FUNCTIONS)
# =================================================================

def segment_object(predictor, image, point):
    predictor.set_image(image)
    input_point = np.array([point])
    input_label = np.array([1])
    masks, scores, logits = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=False,
    )
    return masks[0], scores[0]

def crop_and_prepare_image(image, mask):
    """
    [M·ªöI] √Åp d·ª•ng mask, x√≥a n·ªÅn v√† crop ·∫£nh ƒë·ªÉ chu·∫©n b·ªã cho classifier.
    """
    # T·∫°o ·∫£nh RGBA v√† √°p d·ª•ng mask
    binary_mask = (mask * 255).astype(np.uint8)
    rgba_image = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)
    rgba_image[:, :, 3] = binary_mask

    # T√¨m bounding box c·ªßa v·∫≠t th·ªÉ t·ª´ mask
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) == 0:
        return None, None
        
    x, y, w, h = cv2.boundingRect(contours[0])

    # Crop c·∫£ ·∫£nh RGBA (ƒë·ªÉ hi·ªÉn th·ªã) v√† ·∫£nh RGB g·ªëc (ƒë·ªÉ ph√¢n lo·∫°i)
    cropped_rgba = rgba_image[y:y+h, x:x+w]
    
    # T·∫°o m·ªôt ·∫£nh BGR c√≥ n·ªÅn ƒëen ƒë·ªÉ ƒë∆∞a v√†o feature extractor
    black_background_img = cv2.bitwise_and(image, image, mask=binary_mask)
    cropped_bgr_for_model = black_background_img[y:y+h, x:x+w]
    
    return cropped_rgba, cropped_bgr_for_model

def classify_image_top2(classifier, label_encoder, feature_extractor, image_for_model):
    """
    [UI UPGRADE] Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† ph√¢n lo·∫°i, tr·∫£ v·ªÅ Top 2 k·∫øt qu·∫£.
    """
    resized_img = cv2.resize(image_for_model, (128, 128))
    features = feature_extractor.extract(resized_img)
    if features is None: return None, None
    
    features = features.reshape(1, -1)
    probs = classifier.predict_proba(features)[0]
    
    # L·∫•y 2 index c√≥ x√°c su·∫•t cao nh·∫•t
    top2_indices = np.argsort(probs)[-2:][::-1]
    
    # L·∫•y th√¥ng tin cho Top 1
    pred1_label = label_encoder.classes_[top2_indices[0]]
    pred1_conf = probs[top2_indices[0]]
    
    # L·∫•y th√¥ng tin cho Top 2 (n·∫øu c√≥ ƒë·ªß l·ªõp)
    if len(top2_indices) > 1:
        pred2_label = label_encoder.classes_[top2_indices[1]]
        pred2_conf = probs[top2_indices[1]]
    else:
        pred2_label, pred2_conf = None, None
        
    return (pred1_label, pred1_conf), (pred2_label, pred2_conf)

# =================================================================
# == GIAO DI·ªÜN WEB APP (STREAMLIT UI)
# =================================================================

# --- Sidebar ---
with st.sidebar:
    st.image("https://i.ibb.co/tpsK9NqF/image-2026-01-04-023708095.png", width=100)
    st.title("‚ôªÔ∏è Waste Classifier Pro")
    st.info("D·ª± √°n AI - Ph√¢n lo·∫°i r√°c th·∫£i")
    st.markdown("---")
    
    with st.expander(" H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng", expanded=True):
        st.write("""
        1.  **T·∫£i ·∫£nh:** Nh·∫•n v√†o 'Browse files' v√† ch·ªçn ·∫£nh r√°c th·∫£i c·ªßa b·∫°n.
        2.  **Click:** Di chuy·ªÉn chu·ªôt ƒë·∫øn ·∫£nh g·ªëc v√† **click m·ªôt ƒëi·ªÉm** v√†o gi·ªØa v·∫≠t th·ªÉ b·∫°n mu·ªën ph√¢n lo·∫°i.
        3.  **Xem k·∫øt qu·∫£:** AI s·∫Ω t·ª± ƒë·ªông t√°ch n·ªÅn v√† hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n lo·∫°i b√™n d∆∞·ªõi.
        """)
    
    with st.expander("V·ªÅ d·ª± √°n"):
        st.write("""
        ƒê√¢y l√† s·∫£n ph·∫©m demo k·∫øt h·ª£p:
        - **Meta's SAM:** ƒê·ªÉ t√°ch n·ªÅn v·∫≠t th·ªÉ t·ª± ƒë·ªông.
        - **Stacking Ensemble Model:** (SVM, RandomForest, XGBoost) ƒë·ªÉ ph√¢n lo·∫°i r√°c th·∫£i v·ªõi ƒë·ªô ch√≠nh x√°c cao, ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu t√πy ch·ªânh.
        - **Streamlit:** ƒê·ªÉ x√¢y d·ª±ng giao di·ªán web t∆∞∆°ng t√°c.
        """)

# --- Main Page ---
st.title("‚ú® H·ªá th·ªëng Ph√¢n lo·∫°i R√°c th·∫£i Th√¥ng minh")
st.markdown("T·∫£i l√™n m·ªôt b·ª©c ·∫£nh v√† click v√†o v·∫≠t th·ªÉ ƒë·ªÉ AI t·ª± ƒë·ªông nh·∫≠n di·ªán.")

# T·∫£i models
predictor, classifier, label_encoder, feature_extractor = load_models()
if predictor is None or classifier is None:
    st.stop()

# --- Khu v·ª±c Upload & Canvas ---
uploaded_file = st.file_uploader("Ch·ªçn m·ªôt ·∫£nh r√°c th·∫£i...", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image_bytes = uploaded_file.read()
    st.session_state["original_image"] = Image.open(io.BytesIO(image_bytes)).convert("RGB")

# T√°ch ri√™ng khu v·ª±c v·∫Ω v√† khu v·ª±c k·∫øt qu·∫£
if "original_image" in st.session_state:
    original_pil = st.session_state["original_image"]
    
    st.markdown("---")
    st.subheader("üñºÔ∏è ƒê·∫¶U V√ÄO: Click v√†o v·∫≠t th·ªÉ trong ·∫£nh d∆∞·ªõi ƒë√¢y")
    
    # Canvas cho ph√©p v·∫Ω
    canvas_result = st_canvas(
        fill_color="rgba(255, 165, 0, 0.3)",
        stroke_width=3,
        stroke_color="#FFFF00",
        background_image=original_pil,
        update_streamlit=True,
        height=500, # Set chi·ªÅu cao c·ªë ƒë·ªãnh
        width=750,  # Set chi·ªÅu r·ªông c·ªë ƒë·ªãnh
        drawing_mode="point",
        key="canvas",
    )

    # N·∫øu ng∆∞·ªùi d√πng ƒë√£ click
    if canvas_result.json_data is not None and len(canvas_result.json_data["objects"]) > 0:
        if "last_point" not in st.session_state or st.session_state["last_point"] != canvas_result.json_data["objects"][-1]:
            st.session_state["last_point"] = canvas_result.json_data["objects"][-1]
            
            # L·∫•y t·ªça ƒë·ªô v√† ch·∫°y models
            last_point = st.session_state["last_point"]
            x, y = last_point["left"], last_point["top"]
            
            img_width, img_height = original_pil.size
            canvas_width, canvas_height = 750, 500
            click_point = (int(x * (img_width / canvas_width)), int(y * (img_height / canvas_height)))
            
            original_cv = np.array(original_pil)
            
            with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω... (B∆∞·ªõc 1: T√°ch n·ªÅn, B∆∞·ªõc 2: Ph√¢n lo·∫°i)"):
                mask, score = segment_object(predictor, original_cv, click_point)
                cropped_rgba, cropped_bgr = crop_and_prepare_image(original_cv, mask)
                
                if cropped_bgr is not None:
                    top1, top2 = classify_image_top2(classifier, label_encoder, feature_extractor, cropped_bgr)
                    st.session_state["result"] = (cropped_rgba, score, top1, top2)
                else:
                    st.session_state["result"] = None
    
    st.markdown("---")
    st.subheader("üí° K·∫æT QU·∫¢ PH√ÇN T√çCH")

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u
    if "result" in st.session_state and st.session_state["result"] is not None:
        cropped_rgba, score, top1, top2 = st.session_state["result"]
        
        col_res1, col_res2 = st.columns([1, 2])
        
        with col_res1:
            st.image(cropped_rgba, caption=f"V·∫≠t th·ªÉ ƒë∆∞·ª£c t√°ch (Score: {score:.2f})", use_column_width=True)

        with col_res2:
            # D√πng st.metric ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n
            pred1_label, pred1_conf = top1
            st.metric(label="üèÜ D·ª∞ ƒêO√ÅN H√ÄNG ƒê·∫¶U", value=pred1_label.upper())
            
            # D√πng m√†u s·∫Øc v√† icon ƒë·ªÉ th√¥ng b√°o
            if pred1_conf > 0.8:
                st.success(f"**ƒê·ªô tin c·∫≠y:** {pred1_conf*100:.2f}% (R·∫•t ch·∫Øc ch·∫Øn)")
            elif pred1_conf > 0.6:
                st.info(f"**ƒê·ªô tin c·∫≠y:** {pred1_conf*100:.2f}% (Kh√° ch·∫Øc ch·∫Øn)")
            else:
                st.warning(f"**ƒê·ªô tin c·∫≠y:** {pred1_conf*100:.2f}% (Kh√¥ng ch·∫Øc ch·∫Øn l·∫Øm)")
            st.progress(pred1_conf)
            
            if top2[0] is not None:
                st.markdown("---")
                pred2_label, pred2_conf = top2
                st.metric(label="ü•à L·ª±a ch·ªçn th·ª© hai", value=pred2_label.upper(), delta=f"-{ (pred1_conf - pred2_conf)*100:.1f} %")
                st.write(f"ƒê·ªô tin c·∫≠y: {pred2_conf*100:.2f}%")

    else:
        st.info("Ch∆∞a c√≥ k·∫øt qu·∫£. Vui l√≤ng click v√†o m·ªôt v·∫≠t th·ªÉ tr√™n ·∫£nh.")
else:
    st.info("Vui l√≤ng t·∫£i l√™n m·ªôt b·ª©c ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu.")